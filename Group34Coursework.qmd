---
title: "Group 34 course work"
format:
  pdf:
    toc: true
editor: visual
bibliography: references.bib
bibio-style: unsrt
---

<!-- ## Report -->

<!-- Notes: no more than six figures and/or three tables • \[15 marks\] -->

<!-- Understanding and exploration of both the problem and the data. • \[10 -->

<!-- marks\] Thoroughness and rigour, e.g. clear mathematical description of -->

<!-- models. • \[25 marks\] Clear exposition of the steps you took in model -->

<!-- fitting and exposition of a final model. • \[20 marks\] Clear -->

<!-- presentation and interpretation of results. • \[10 marks\] Critical -->

<!-- review of the analysis. • \[20 marks\] Clarity and conciseness in -->

<!-- writing and tidy presentation of R code and associated plots. -->

{{< pagebreak >}}

## Introduction:

Tuberculosis (TB) is a bacterial disease caused by the bacterium
Mycobacterium tuberculosis. It primarily affects the lungs, but can also
affect other parts of the body, such as the kidneys, spine, and brain.

TB is a major public health problem worldwide, affecting millions of
people each year. According to the World Health Organization (WHO), TB
is one of the top 10 causes of death worldwide, and in 2020 alone, there
were an estimated of approximately 10 million cases of TB reported
globally. Brazil is one of the countries with a high burden of TB with
an estimate of 96000, and it is considered a priority country for TB
control by the WHO. @who_tb_2020.

The purpose of this report is to provide a comprehensive analysis of TB
in Brazil from the 2012 to 2014 data, including an overview of the
trends and risk factors associated with TB. Additionally, this report
aims to highlight the most important factor for improving TB control and
prevention efforts in Brazil.

The case study we will consider in this report is the Tuberculosis
dataframe where Brazil is divided into 557 administrative microregions
and the available data comprises of counts of TB cases in each
microregion from 2012 to 2014.

## Exploratory Data Analysis:

<!-- Provide a summary of the Tuberculosis dataset, including descriptive statistics and visualizations. -->
<!-- Identify any patterns or trends in the data, such as geographical or temporal clusters of cases. -->
<!-- Discuss any issues with the data, such as outliers or inconsistencies. -->

An exploratory analysis of this dataset can reveal important insights of
the potential risk factors for TB in Brazil and help guide public health
interventions.

The TBdata dataframe contains information on various socio-demographic
and geographic factors in Brazil that may be associated with TB
incidence in each microregion. These factors include indigenous
population, illiteracy levels, urbanization rate, dwelling density,
poverty levels, sanitation levels, unemployment rates, and timeliness of
TB case reporting. The dataset also includes information on the number
of TB cases and population size for each microregion, as well as unique
ID numbers to distinguish between the different regions.

We will start by analysing the distributions of each of the variables to
identify any patterns ![Initial variable
analysis](histogramExploratoryAnalysis.png)

Firstly, the dwelling density seems to follow a normal distribution that
is skewed to the right and a mean of approximately 0,6.

Secondly illiteracy is very heavily skewed to the right but it still
displays a normal bell curve around the 5% illiteracy level.

Poor sanitation is extremely skewed to the right.

Unemployment seems to follow a normal distribution with little to no
skewness and a mean of approximately 6%.

The other values do not seem to follow any easily identifiable trend nor provide us any meaningful insights with their distribution alone.

![Correlation Matrix](HeatMap.png)

As we can see from the matrix, the variable that is the most correlated
from TB is the population, with illiteracy, poor sanitation and poverty
having a negative correlation with TB.

## Model Selection:

<!-- Explain the process used to select the variables to include in the GAM model. -->

<!-- Discuss any variable transformations or interactions that were considered. -->

<!-- Discuss any model assumptions that were evaluated, such as the normality of the residuals, the linearity of the predictor variables, and the spatial and temporal autocorrelation. -->

We are utilising generalized additive models (GAMs) in the case study of
TB in Brazil as it can model complex and non-linear relationships between
TB incidence and risk factors, control for relevant covariates, identify
important predictors of TB incidence, and predict TB incidence in
different regions of Brazil.

<!-- Generalized linear models (GLMs) are a type of regression model that assumes a linear relationship between the dependent variable and the independent variables. While GLMs can be useful in many scenarios, they may not be appropriate for modeling complex and nonlinear relationships, such as those seen in the case of tuberculosis (TB) in Brazil. //This one is for me personally -->

As the data is count data we will first fit a Poisson module since this
distribution is a good fit for the nature of the data

Yi ∼ Pois($\lambda$) Yi indep.

Log($\lambda i$) =
$log(Populationi)+fIndigenous(Indigenousi)+fIlliteracy(Illiteracyi)+fUrbanisation(Urbanisationi)+fDensity(Densityi)+fPoverty(Povertyi)+fPoor_Sanitation(Poor_Sanitationi)+fUnemployment(Unemploymenti)+fTimeliness(Timelinessi)+fla(lati)+flon(loni)+fYear(Yeari)+flon,lat,Year(loni,lati,Yeari)$



## Model Fitting:

<!-- Explain the process used to fit the GAM model, such as the choice of smoothing functions, the type of penalty used, and the optimization algorithm used to estimate the parameters. -->

<!-- Describe the hyperparameters used, such as the degree of smoothing or the type of penalty used. -->

![QQ Plot](QQ.png)

Our QQ-plot suggest that the quantiles in our data our not similar to
the line as it deviates from the line in nearly all the values, showing
a very flawed fit. As this suggests that our current model doesn't fit
the data correctly and required an extension to our model as the Poisson
GAM is not accounted for enough deviance as seen in the residuals.

Since the model is not accounting for enough of the variance we will
check if there is a significant difference between the variance and the
mean. In this analyses we will use the Pearson estimate for the
dispersion parameter, this method allow us to estimate the amount of
extra variability, or over-dispersion in count data and therefore
analyse if the Poisson distribution assumption of equal mean and
variance holds.

However the Poisson model seems to not have a good enough fitting
results and as such we will extend the model into a negative binomial
GAM

## Model Evaluation:

<!--Provide a summary of the goodness-of-fit measures used to evaluate the model, such as the explained variance, the deviance, and the residual analysis. -->

<!--Discuss any limitations or assumptions of the GAM model, such as the potential for overfitting or the sensitivity to the choice of smoothing parameters. -->

<!--Compare the GAM model to other models that were considered, such as a generalized linear model(GLM). -->

## TODO make this mathematical istead of the code

Calulating the dispersion parameter: $\sum_{k=0}^{n} = 6.67687$
sum(residuals(poisson_model, type = "pearson")\^2) / df.residual(poisson_model) = 6.67687
$$\hat{\phi}{\text{Pearson}}=\frac{1}{n-p}\sum{i=1}^{n}\frac{(y_i-\hat{y}_i)^2}{v(\hat{y}_i)}$$

As we can see from the dispersion parameter should be 1 for the
assumption of equal mean and variance to hold true, so it seems that
there is substantial over-dispersion in the Poisson GAM. This violates
one of the Poisson assumptions that the mean and variance are equal
therefore we will have to extend the model from e GAM Poisson to a
Negative Binomial GAM

As we can see from the residual versus predictor plot, the values seem
to be randomly scattered with no clear trend but with some distance from
the zero line. As such we can determine that this scatter is due to
random errors and not a unacounted patern in the model.

![QQ - Negative Binomial](QQNB.png)

The QQ-plot looks much better for the Negative Binomial model. The
majority of points lie either on top of very near the y=x line, except
for a few towards the extremes. This indicates our assumption about the
true distribution of the data is a lot more safe than it was before.

## TODO add the residual sum here

$sum$
sum(residuals(nb_model, type = "pearson")\^2) / df.residual(nb_model)

The dispersion parameter is very close to 1, unlike for the Poisson
model, meaning that the model that can account for most of the
over-dispersion in the data. As such a dispersion parameter value close
to 1 can be interpreted as the model is a good fit for the data due to
the model adequately capture the variability of the the response
variable.

## Results and Interpretation:

<!-- Present the results of the GAM model, including the coefficients and their confidence intervals. -->

<!-- Interpret the coefficients and their practical significance. -->

<!-- Provide visualizations of the model predictions and any interactions or nonlinear relationships. -->

<!-- Summarize the findings of the GAM model. -->




## Apendix

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), echo=FALSE, output=FALSE}
# Loading packages
library(fields)
library(maps)
library(sp)
library(mgcv)
library(ggplot2)
library(GGally)


load("datasets_project.Rdata")

# install.packages("DataExplorer")
library(DataExplorer)

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}
## PLotting map of cases
plot.map(TBdata$TB[TBdata$Year==2014],n.levels=7,main="TB counts for 2014")


```

## Exploratory analyses

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), echo=FALSE, eval=FALSE}

##plot(TBdata) looks terrible

## this one is great but it becomes its own pdf so I will be using the individual functions from the package
create_report(TBdata)

```

```{r, warning=FALSE, message = FALSE, echo=FALSE, eval=FALSE}
# Set the figure size and margin
options(repr.plot.width = 10, repr.plot.height = 8)

# Select the variables of interest
vars <- c("Indigenous", "Illiteracy", "Urbanisation", "Density", "Poverty", 
          "Poor_Sanitation", "Unemployment", "Timeliness", "TB", "Population")

# Create the scatterplot matrix
ggpairs(TBdata[, vars])
```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}
plot_histogram(TBdata)

```

//TODO talk about the histograms and relevant distributions we can
observe

Now investigating the correlation matrix of the numerical variables

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

plot_correlation(TBdata)

```

As we can see from the matrix, the variable that is the most correlated
from TB is the population, with illiteracy, poor sanitation and poverty
having a negative correlation with TB.

## Poisson definition

As the data is count data we will first fit a Poisson module since this
distribution is a good fit for the nature of the data

```{r}
poisson_model <- gam(TB ~ offset(log(Population)) + s(Indigenous, k = 20) + s(Illiteracy , k = 20) + s(Urbanisation, k = 20) + s(Density, k = 20) + s(Poverty, k = 20) + s(Poor_Sanitation, k = 20) + s(Unemployment, k = 20) + s(Timeliness, k = 20) + s(lat, k = 30) + s(lon, k = 30) + s(Year, k = 3) + ti(lon, lat, Year, k = 3), family = poisson, data = TBdata, method = 'REML')
summary(poisson_model)
```

```{r, eval=FALSE}
gam.check(poisson_model)
```

<!-- ## Separate the plots the output is too messy -->

```{r}
#Akaike Information Criterion:
poisson_model$aic

plot(poisson_model, shade=T, rug = TRUE, residuals = TRUE,
pch = 1, cex = 0.5)

#Lets check our model residuals:
qq.gam(poisson_model)
```

Our QQ-plot suggest that the quantiles in our data our not similar to
the line as it deviates from the line in nearly all the values, showing
a very flawed fit. As this suggests that our current model doesn't fit
the data correctly and required an extension to our model as the Poisson
GAM is not accounted for enough deviance as seen in the residuals.

<!-- ## TODO better prepare the residuals and explain them if we have enough figures to show -->

Since the model is not accounting for enough of the variance we will
check if there is a significant difference between the variance and the
mean. In this analyses we will use the Pearson estimate for the
dispersion parameter, this method allow us to estimate the amount of
extra variability, or over-dispersion in count data and therefore
analyse if the Poisson distribution assumption of equal mean and
variance holds.

```{r}
#Calculating Pearson estimate for dispersion parameter using Pearson residuals:
sum(residuals(poisson_model, type = "pearson")^2) / df.residual(poisson_model)

#The dispersion parameter should be 1, so it seems that there is substantial over-dispersion in the Poisson GAM.
```

As we can see from the dispersion parameter should be 1 for the
assumption of equal mean and variance to hold true, so it seems that
there is substantial over-dispersion in the Poisson GAM. This violates
one of the Poisson assumptions that the mean and variance are equal
therefore we will have to extend the model from e GAM Poisson to a
Negative Binomial GAM

## Negatice binomial

## maybe make discrete things like years, lat,long as factors
```{r}
#fitting a negative-binomial model to our TB data:
nb_model <- gam(TB ~ offset(log(Population)) + s(Indigenous, k = 20) + s(Illiteracy , k = 20) + s(Urbanisation, k = 20) + s(Density, k = 20) + s(Poverty, k = 20) + s(Poor_Sanitation, k = 20) + s(Unemployment, k = 20) + s(Timeliness, k = 20) + s(lat, k = 30) + s(lon, k = 30) + s(Year, k = 3) + ti(lon, lat, Year, k = 3), negbin(theta = 9, link = "log"), data = TBdata, method = 'REML')

summary(nb_model)
```

```{r}
#Akaike Information Criterion
nb_model$aic
```

As we can see from this Akaike Information Criterion(AIC) the Negative
Binomial has a significantly lower value than the previous 18585,52 from
the GAM Poisson, meaning this is already a better fitting model than the
previous one.

Now we will check the residuals to check for any anomalies on our model
prediction

```{r}
gam.check(nb_model)
```
## TODO year is being overfit because K -edf is less than 1

As we can see from the residual versus predictor plot, the values seem
to be randomly scattered with no clear trend but with some distance from
the zero line. As such we can determine that this scatter is due to
random errors and not a unacounted patern in the model.

<!-- ##TODO double check this last claim with matthew -->

```{r}
#checking the model residuals
qq.gam(nb_model)
```

The QQ-plot looks much better for the Negative Binomial model. The
majority of points lie either on top of very near the y=x line, except
for a few towards the extremes. This indicates our assumption about the
true distribution of the data is a lot more safe than it was before.

```{r}
#Calculating Pearson estimate for dispersion parameter using Pearson residuals:
sum(residuals(nb_model, type = "pearson")^2) / df.residual(nb_model)
```

The dispersion parameter is very close to 1, unlike for the Poisson
model, meaning that the model that can account for most of the
over-dispersion in the data. As such a dispersion parameter value close
to 1 can be interpreted as the model is a good fit for the data due to
the model adequately capture the variability of the the response
variable.

## Again tidy up the plots of the Negative Binomial

```{r}
plot(nb_model, shade=T, rug = TRUE, residuals = TRUE,scheme=1,
pch = 1, cex = 0.5)
```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

plot(nb_model, shade=T, rug = TRUE, residuals = TRUE,scheme=2,
pch = 1, cex = 0.5)

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

TBdata$risk = (TBdata$TB +0.00000001 )/TBdata$Population

TBdata$risk

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

par(mfrow = c(1,3))
plot.map(TBdata$TB[TBdata$Year==2012],n.levels=7,main="TB counts for 2012")
plot.map(TBdata$TB[TBdata$Year==2013],n.levels=7,main="TB counts for 2013")
plot.map(TBdata$TB[TBdata$Year==2014],n.levels=7,main="TB counts for 2014")
par(mfrow = c(1,1))

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}



```
