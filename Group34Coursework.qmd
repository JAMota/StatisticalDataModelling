---
title: "Group 34 course work"
format:
  pdf:
    toc: true
editor: visual
bibliography: references.bib
bibio-style: unsrt
quarto:
  figure-toc: true
  figure-toc-title: "Table of Figures"
  figure-toc-depth: 2
---

<!-- ## Report -->

<!-- Notes: no more than six figures and/or three tables • \[15 marks\] -->

<!-- Understanding and exploration of both the problem and the data. • \[10 -->

<!-- marks\] Thoroughness and rigour, e.g. clear mathematical description of -->

<!-- models. • \[25 marks\] Clear exposition of the steps you took in model -->

<!-- fitting and exposition of a final model. • \[20 marks\] Clear -->

<!-- presentation and interpretation of results. • \[10 marks\] Critical -->

<!-- review of the analysis. • \[20 marks\] Clarity and conciseness in -->

<!-- writing and tidy presentation of R code and associated plots. -->

{{< pagebreak >}}


## Introduction:

Tuberculosis (TB) is a bacterial disease that primarily affects the
lungs but can also impact other parts of the body. It is a significant
public health problem, with approximately 10 million cases reported
globally in 2020. Brazil is a high-burden country for TB, with an
estimated 96000 cases. The purpose of this report is to determine
whether various socio-economic variables impact the rate of TB per unit
population in Brazil between 2012 and 2014. We will analyse data from
537 micro-regions in Brazil, including the latitude and longitude of
each region and the year in which the data was collected. @who_tb_2020.

The socio-economic variables that were recorded for each micro-region in
our data set are as follows: the level of illiteracy, urbanisation,
poverty, unemployment, and sanitation; the proportion of indigenous
population; the dwelling density; and finally, a proxy indicator of the
amount of resources in the form of the average amount of time between
diagnosing a TB case and reporting it to the health system. In addition,
the latitude and longitude of the respective 537 micro-regions, as well
as the year in which the data was obtained, are supplied for each of
these values. Because of this, we will be better able to explain the
geographical, temporal, and spatio-temporal structure of any systematic
risk that is not described by the covariates.

## Exploratory Data Analysis:

A simple exploration of our covariates and their potential relationships
with the rate of tuberculosis in each microregion of Brazil was carried
out before attempting any type of formal statistical analysis or
regression on the data. Several of the findings were unexpected, such as the
observation that a lower degree of sanitation did not appear to be
associated with a higher rate of tuberculosis cases. The same was true
in regard to the levels of poverty. Nevertheless, the issue with
attempting to infer statistical associations in such a straightforward
manner is that we are unable to take into consideration the possibility
of changes occurring in other variables for each of the data points.
This is the primary reason why we need to use a formal model to
investigate the impact of our covariates on the incidence of
tuberculosis in relation to the total population.

@fig-Analysis

When developing statistical models, there is a trade-off between interpretability and flexibility. Linear models are often preferred due to their simplicity and ease of interpretation, but they may not accurately represent complex non-linear relationships in the data, such as in the case of tuberculosis data. On the other hand, machine learning models like neural networks or boosted trees can provide accurate predictions but are difficult to interpret and require a large amount of data. As a middle ground, generalized additive models (GAMs) were chosen as the preferred framework for analysis in this case. GAMs provide a balance between interpretability and flexibility, making them a suitable choice for this analysis.

@fig-Brazil

## Model Selection:

At first, the idea was to combine a log link and a Poisson generalised
additive model. This was because we were working with count data in the
form of TB cases broken down by each microregion. Nonetheless, it was
essential to standardise this count because the populations in each
region were distinct from one another. After doing some study on the
topic, we discovered that using the log of the population in the model
is the most effective way to carry out a population standardised
regression. While we were trying to fit our model in R, it was necessary
for us to include an offset for the log of the population. This provides
us with the tuberculosis rate per capita:

Let the model be : <br>
$$ Log(\lambda_i) = offset(log(Population_i)) + f_1(Indigenous_i) + $$
<br>
$$ f_2(Illiteracy_i) + \\f_3(Urbanisation_i) + f_4(Density_i) + f_5(Poverty_i) + $$
<br>
$$ f_6(PoorSanitation_i) +\\ f_7(Unemployment_i) + f_8(Timeliness_i) + $$
<br>
$$ f_9(Year_i) + f_{10}(Year_i, lon_i) + \\ f_{11}(lat_i,Year_i), $$
<br> $$ {u_i} = E[TB_i] ~~ and  ~~ TB_i \sim Pois(\lambda_i) $$

Confirming that our model was accurate was the next step in the process
of developing our model. Unfortunately, a QQ-plot revealed that the
quantiles in our data did not closely resemble what they would look like
if they followed a theoretical poisson distribution. This was the
conclusion that could be drawn from the plot. After calculating a
Pearson estimate of our dispersion parameter to determine whether or not
this lack of fit was due to over-dispersion, the results were
unequivocal: the parameter was almost 9,33 (when it should have been 1),
which indicated that our data was indeed over dispersed for a Poisson
model.

An alternative model to Poisson is a Negative Binomial model. The
Negative Binomial (NB) model is likewise suitable for use with count
data; however, it differs from the Poisson regression in that it
contains an additional parameter that can alter the variance
independently from the mean.

<!-- Let the model be : <br> -->

<!-- $$ Log(\lambda_i) = offset(log(Population_i)) + f_1(Indigenous_i) + f_2(Illiteracy_i) + \\f_3(Urbanisation_i) + f_4(Density_i) + f_5(Poverty_i) + f_6(PoorSanitation_i) +\\ f_7(Unemployment_i) + f_8(Timeliness_i) + f_9(Year_i) + f_{10}(Year_i, lon_i) + \\ f_{11}(lat_i,Year_i),\\ {u_i} = E[TB_i] and TB_i \sim NB(r,p) $$ -->

Because of this, it is more flexible than the Poisson model, and as a
result, it can fit data with a greater degree of fluctuation. A NB model
was fitted to the data making use of the exact same specification as was
used before. We can count ourselves fortunate that the QQ-plot5 for the
revised model showed a good fit (the majority of the dots fell on the
y=x line), and the estimate for the dispersion parameter was 1,133. The
AIC was also reduced for our newly developed model. We arrived at the
following model after making adjustments to the choice of rank for each
of our smooth terms (covariates), until we were certain that they were
not too low and edf was not too similar to k'.

INSERT BOTH QQ PLOT HERE, FIGURE -2 at the last of the code.

## Model Fitting and Results:

The most difficult aspect of using this model was figuring out how to
account for the interplay that exists between time and place. It was
required to add space and time as a product smooth, despite the fact
that the majority of the other covariates were modelled using univariate
smooth functions. After doing some research on the topic, we came to the
conclusion that the interaction term should be incorporated into the
model as a tensor product smooth utilising the "te" term in the mgcv
package. Because space and time are measured on such distinct scales,
this particular sort of interaction works best in situations in which
the two important variables are on different scales.

The negative binomial gam model output shows the statistical
significance of various predictors on the incidence of tuberculosis (TB)
in a given population. The model assumes a negative binomial
distribution with a log link function, indicating that the response
variable, TB, has a count distribution and that the logarithm of the
expected counts is modeled as a linear combination of the predictors.

The intercept term in the model is statistically significant (p\<0.001),
indicating that there is a significant difference in the TB incidence
rate even when all other predictor variables are set to zero. The
negative coefficient on the intercept suggests that there is a baseline
level of protection against TB in the population.

Among the predictors, all except s(Year) are statistically significant
at the 0.05 level. Indigenous status, Illiteracy, Poverty, and
Timeliness have a statistically significant positive association with
the incidence of TB. Urbanisation, Density, Poor Sanitation, and
Unemployment are also significantly associated with TB but have a
negative effect on incidence.

The s(Year) term has a statistically significant association with TB
incidence but its effect is not linear. The term is modeled using a
spline with a small number of degrees of freedom (k=3). The p-value for
the term is \<2e-16, indicating a strong association with TB incidence.

The interaction terms, te(lon,Year) and te(lat,Year), which model the
effect of longitude and latitude on TB incidence with respect to year,
respectively, are also statistically significant. Both terms have
positive effects on TB incidence, indicating that the risk of TB
increases with increasing longitude or latitude, depending on the year.

Overall, the model has an adjusted R-squared value of 0.895, indicating
that the predictors in the model explain 53.4% of the variability in TB
incidence. The scale estimate is 1, suggesting that the model is
well-calibrated. The negative binomial distribution is appropriate for
modeling the count response variable, and the log link function is
appropriate for modeling the logarithm of the expected counts.

## Model Evaluation and Discussion:

<!-- HERE DISCUSS THE FIGURES -->

<!-- temporally the only variable that changes is the TB cases and population per region -->

<!-- 1.  TEMPORAL -->

<!-- 2.  SPATIAL -->

<!-- 3.  SPATIAL-TEMPORAL -->

First, lets discuss the spatial findings of our analysis. As we can see
from the yearly graph, @fig-TemporalAnal, the analysis shows that there
is a gradual decrease of TB cases per capita from 2012 to 2013 at the
national level. In contract from 2013 to 2014 the data displays very
little decrease of TB risk on a national level.

Secondly, our spatial analysis discovered that the risk of Tb increases
the more you deviate from the geological center of Brazil, with the risk
of Tb <!-- going from 3.6 to 4.8 --> increasing up to 33% as we go from
the center-east region of Brazil to its borders on west side including
as such a big portion of the Amazonian region. However as it can be
observed from the figure there is a much bigger increase in TB risk as
we move horizontally then vertically.

@fig-SpatialAnal


Thirdly, merging the 2 previous analysis into the spatio-temporal we can
see a more significant decrease of cases overall from 2012 to 2013 in
all microregions. In the geographical center of Brazil and the Amazonian
outskirts in both years Where the coastal regions have a some
improvements from 2012 to 2013 but have either maintained or increased
their TB risk from 2013 to 2014

@fig-SpatialTemporalAnal and 7

## Conclusion:

In summary, the gam model identifies several significant predictors of
TB incidence, including Indigenous status, Illiteracy, Poverty, and
Timeliness, as well as Urbanisation, Density, Poor Sanitation, and
Unemployment. The interaction terms with latitude and longitude also
play a significant role in predicting TB incidence. The model can be
used to identify populations at risk for TB and to develop interventions
to reduce TB incidence in these populations.

In conclusion, we discuss some of the restrictions imposed in this
study. The relatively little time frame under consideration presents the
most evident limitation of the study. If determining any temporal
structure of systematic risk was the objective, then additional time
ought to elapse prior to the data being examined and modelled in order
to allow for the passage of time. Because the incidence and rate of
tuberculosis are probably affected by a wide variety of other factors
that are not accounted for in our dataset, additional covariates could
also be added to the study. Additional limitations of our study are
associated with the application of a generalised additive model (GAM),
including the risk that our model overfits our data, as well as its high
computing cost and complexity.

## Figures

![Brazil TB exploratory Analysis](Exploration.png){#fig-Analysis}

![QQ Plot](QQ.png){#fig-QQ-plot}

![Temporal Analysis of risk of TB per Year - fig 4](Temporal.png){#fig-TemporalAnal}

![Spatial Analysis of risk of TB per Year per region -fig 5](Spatial.png){#fig-SpatialAnal}

![Temporal-Spatial Analysis of risk of TB per 100000 per Year per region -fig 6](SpatialTemporal.png){#fig-SpatialTemporalAnal}

if we have a spare 
![Temporal-Spatial Analysis of risk of TB per 100000 per Year per region -fig 7](TemporalSpatial.png){#fig-Brazil}

## Code Appendix

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), echo=FALSE, output=FALSE}

# Loading packages
library(fields)
library(maps)
library(sp)
library(mgcv)
library(ggplot2)
library(GGally)
library(xtable)

load("C:/AppliedDataScienceAndStatistics/StatisticalDataModelling/datasets_project.RData")

# install.packages("isadug ") package for latex table conversion
library(DataExplorer)



```

## time series graph

```{r, fig.height= 4, fig.width=8}

TBdata$Risk = (TBdata$TB / TBdata$Population) * 100000

## PLotting map of cases
par(mfrow = c(1,3))
plot.map(TBdata$TB[TBdata$Year==2012],n.levels=7,main="TB counts for 2012")
plot.map(TBdata$TB[TBdata$Year==2013],n.levels=7,main="TB counts for 2013")
plot.map(TBdata$TB[TBdata$Year==2014],n.levels=7,main="TB counts for 2014")

## PLotting map of risk
par(mfrow = c(1,3))
plot.map(TBdata$Risk[TBdata$Year==2012],n.levels=7,main="TB regional risk per 100k for 2012")
plot.map(TBdata$Risk[TBdata$Year==2013],n.levels=7,main="TB regional risk per 100k for 2013")
plot.map(TBdata$Risk[TBdata$Year==2014],n.levels=7,main="TB regional risk per 100k for 2014")



```
This last one if for the temporal-spacial analysis 


## Exploratory analyses

```{r, warning=FALSE, message = FALSE, echo=FALSE, eval=FALSE, fig.height=6, fig.width=6}
# INSERT THIS IN THE REPORT AS Figure-1
# Set the figure size and margin
options(repr.plot.width = 10, repr.plot.height = 8)

# Select the variables of interest
vars <- c("TB","Indigenous", "Illiteracy", "Urbanisation", "Density", "Poverty", 
          "Poor_Sanitation", "Unemployment", "Timeliness", "Population")

# Create the scatterplot matrix
ggpairs(TBdata[ ,vars])
```

```{r}
# INSERT THIS AS SUMMARY TABLE AS TABLE-1
summary_table <- summary(TBdata)
# Convert the summary table to a LaTeX table using the xtable() function
latex_table <- xtable(summary_table)

# Print the LaTeX table to the console
print(latex_table)
```

As we can see from the matrix, the variable that is the most correlated
from TB is the population, with illiteracy, poor sanitation and poverty
having a negative correlation with TB.

## Poisson Model definition

As the data is count data we will first fit a Poisson module since this
distribution is a good fit for the nature of the data

```{r}
poisson_model <- gam(TB ~ offset(log(Population)) + s(Indigenous, k = 20) + s(Illiteracy , k = 20) + s(Urbanisation, k = 20) + s(Density, k = 20) + s(Poverty, k = 20) + s(Poor_Sanitation, k = 20) + s(Unemployment, k = 20) + s(Year, k=3) + s(Timeliness, k = 20) +te(lon, Year, k = 3)+ te(lat, Year, k = 3), family = poisson, data = TBdata, method = 'REML')
summary(poisson_model)
```

```{r, eval=FALSE}
gam.check(poisson_model)
```

<!-- ## Separate the plots the output is too messy -->

```{r}
#Akaike Information Criterion:
poisson_model$aic

plot(poisson_model, shade=T, rug = TRUE, residuals = TRUE,
pch = 1, cex = 0.5)

```

Our QQ-plot suggest that the poisson model fit deviates from the
theoretical quantiles in nearly all the values, showing a very flawed
fit. As this suggests that our current model doesn't fit the data
correctly and required an extension to our model as the Poisson GAM is
not accounted for enough deviance as seen in the residuals.

<!-- ## better prepare the residuals and explain them if we have enough figures to show -->


Since the model is not accounting for enough of the variance we will
check if there is a significant difference between the variance and the
mean. In this analyses we will use the Pearson estimate for the
dispersion parameter, this method allow us to estimate the amount of
extra variability, or over-dispersion in count data and therefore
analyse if the Poisson distribution assumption of equal mean and
variance holds.

```{r}
#Calculating Pearson estimate for dispersion parameter using Pearson residuals:
sum(residuals(poisson_model, type = "pearson")^2) / df.residual(poisson_model)

#The dispersion parameter should be 1, so it seems that there is substantial over-dispersion in the Poisson GAM.
```

As we can see from the dispersion parameter should be 1 for the
assumption of equal mean and variance to hold true, so it seems that
there is substantial over-dispersion in the Poisson GAM. This violates
one of the Poisson assumptions that the mean and variance are equal
therefore we will have to extend the model from e GAM Poisson to a
Negative Binomial GAM

## Negative binomial

```{r}
#fitting a negative-binomial model to our TB data:
nb_model <- gam(TB ~ offset(log(Population)) + s(Indigenous, k = 20) + s(Illiteracy , k = 20) + s(Urbanisation, k = 20) + s(Density, k = 20) + s(Poverty, k = 20) + s(Poor_Sanitation, k = 20) + s(Unemployment, k = 20) + s(Year, k=3) + s(Timeliness, k = 20) +te(lon, Year, k = 3)+ te(lat, Year, k = 3), negbin(theta = 9, link = "log"), data = TBdata, method = 'REML')

summary(nb_model)
```

```{r}
#Akaike Information Criterion
nb_model$aic
```

As we can see from this Akaike Information Criterion(AIC) the Negative
Binomial has a significantly lower value than the previous 18585,52 from
the GAM Poisson, meaning this is already a better fitting model than the
previous one.

Now we will check the residuals to check for any anomalies on our model
prediction

```{r}
gam.check(nb_model)
```

As we can see from the residual versus predictor plot, the values seem
to be randomly scattered with no clear trend but with some distance from
the zero line. As such we can determine that this scatter is due to
random errors and not a uncounted pattern in the model.



```{r}
plot(nb_model, shade=T, rug = TRUE, residuals = TRUE,
pch = 1, scheme =1,  cex = 0.5)
```

The QQ-plot looks much better for the Negative Binomial model. The
majority of points lie either on top of very near the y=x line, except
for a few towards the extremes. This indicates our assumption about the
true distribution of the data is a lot more safe than it was before.

```{r}
#Calculating Pearson estimate for dispersion parameter using Pearson residuals:
sum(residuals(nb_model, type = "pearson")^2) / df.residual(nb_model)
```

The dispersion parameter is very close to 1, unlike for the Poisson
model, meaning that the model that can account for most of the
over-dispersion in the data. As such a dispersion parameter value close
to 1 can be interpreted as the model is a good fit for the data due to
the model adequately capture the variability of the the response
variable.


```{r}
plot(nb_model, shade=T, rug = TRUE, residuals = TRUE,scheme=1,
pch = 1, cex = 0.5)
```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

plot(nb_model, shade=T, rug = TRUE, residuals = TRUE,scheme=2,
pch = 1, cex = 0.5)

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72, fig.width = 12)}
par(mfrow= c(1,2))

qq.gam(poisson_model, main = 'Q-Q Plot for Poisson Model')
qq.gam(nb_model, main = 'Q-Q Plot for Negative Binomial Model')
```
Just the space and time graphs needed
```{r, eval=FALSE}

par(mfrow= c(1,3))

plot(nb_model, shade=T, rug = TRUE, residuals = TRUE,scheme=1,
pch = 1, cex = 0.5)

par(mfrow = c(1, 1))

```

##spatial graph does not render automaticall have to be added manually

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72, fig.width = 12), eval=FALSE}
par(mfrow=c(3,1))
vis.gam(nb_model,view=c("lon","lat"),cond=list(Year=2012),plot.type = "contour")
vis.gam(nb_model,view=c("lon","lat"),cond=list(Year=2013),plot.type = "contour")
vis.gam(nb_model,view=c("lon","lat"),cond=list(Year=2014),plot.type = "contour")
par(mfrow=c(1,1))

## a spatial graph independent of the years
vis.gam(nb_model,view=c("lon","lat"),plot.type = "contour")

```

## Bibliography
